{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000000/100000000 [00:10<00:00, 9377087.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  10.666391134262085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "max = 100000000\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(max)):\n",
    "    if i == max:\n",
    "        print(\"done\")\n",
    "        break\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken: \", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data shape:  torch.Size([1, 1, 28, 28])\n",
      "Transformed target shape:  torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # test transform with dummy data\n",
    "dummy_data = torch.rand((1, 28, 28))\n",
    "dummy_target = torch.tensor([1])\n",
    "\n",
    "dummy_dataset = [(dummy_data, dummy_target)]\n",
    "\n",
    "dummy_loader = torch.utils.data.DataLoader(dummy_dataset, batch_size=1, shuffle=True)\n",
    "for data, target in dummy_loader:\n",
    "    print(\"Transformed data shape: \", data.shape)\n",
    "    print(\"Transformed target shape: \", target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (normalize pixel values from 0-255 to 0-1)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "\n",
    "# Load training data\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Load test data\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 64.94it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAADgCAYAAAD19b5rAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHndJREFUeJzt3QmUFNXVAOAaBEUWDYui4H5QAUFiQFHiEsQoiQuiYoQIMYqgkmgMxghCSEBRo6jRoGhckCXuYFCMcuKWmAhK3IKAUcEFVxTX4MLS/6n+DypSb6CGrumZ7u87h0y8r1/Vm55+XX2rqt+tyOVyuQgAAADIRJ1sNgsAAADEJN4AAACQIYk3AAAAZEjiDQAAABmSeAMAAECGJN4AAACQIYk3AAAAZEjiDQAAABmSeAMAAECGJN4Ze/nll6OKiorokksuKdg2H3744fw2459Q25gTsCZzAtZkTsCazInSIPFOMGHChPwLcc6cOVEp+u1vf5v//b75r379+sUeGjVUqc+JHXbYIXFOxP923nnnYg+PGqjU50Ts9ddfj4499tjoW9/6VrTZZptFPXv2jBYuXFjsYVFDlfqcmDZtWnTIIYdELVu2jDbZZJNom222iY455pho7ty5xR4aNVSpz4nVbr311mifffaJGjZsmD9edO3aNXrwwQeLPawaqW6xB0DxXH311VGjRo2+/O+NNtqoqOOBYrn88sujTz75ZI3YK6+8Eg0fPjw6+OCDizYuKJZ4PnTr1i368MMPo2HDhkX16tWLLrvssuiAAw6Inn766ahZs2bFHiJUq//85z9RkyZNojPOOCNq3rx59NZbb0U33HBDtNdee0WPPfZY1LFjx2IPEYpyMW/UqFH5k1AnnHBCtHz58vzJqPjELWuTeJexeJLEBw8od0ceeeRasfPOOy//88c//nERRgTFddVVV0UvvPBC9Pjjj0d77rlnPvaDH/wgat++fTR27NhozJgxxR4iVKvf/OY3a8UGDBiQv/IdX8gYP358UcYFxTJr1qx80h0fE84888xiD6dWcKt5FX3xxRf5N+FOnTpFm2++ef72iv322y966KGHgn3iqwXbb799tOmmm+avGiTdnrRgwYJ8Qty0adP8rd+dO3eOpk+fvs7xLFu2LN/33XffXe/fIZfLRR999FH+J2yoUpgTX/fnP/852nHHHfO3TEG5zYk77rgjn3CvTrpjbdq0ibp37x7ddttt6+wPpTYnkmy55ZZRgwYNog8++KBK/aE2z4n4bsGtttoqfxdInEt8885B1ibxrqI4Yb3uuuui733ve9FFF12Uv9ViyZIl+e//xLfhfdPEiROjK664Iho8eHA0dOjQ/CQ58MADo7fffvvLxzz33HPR3nvvHc2fPz8655xz8meQ4gkYX42Lv1tUmfiqRNu2baM//vGP6/077LTTTvlJ3rhx4+j4449fYyxQjnNitaeeeiq/z759+6buC7V9TqxatSp69tln8x/Uvim+rfall16KPv7441TPBdTmOfF1cZIdjzm+9Ty+4h3/TvEJKSi3OfHAAw/kT87G49liiy3y+cTWW29dpc9dZSPHWm688cb4EnDuiSeeCD5mxYoVuc8//3yN2Pvvv59r0aJF7sQTT/wytmjRovy2Nt1009zixYu/jM+ePTsfP/PMM7+Mde/ePdehQ4fcZ5999mVs1apVua5du+Z23nnnL2MPPfRQvm/885uxkSNHrvP3u/zyy3M/+9nPclOmTMndcccduTPOOCNXt27d/D4+/PDDdfan/JT6nPimIUOG5PvOmzcvdV/KQynPiSVLluQfN2rUqLXaxo0bl29bsGBBpdug/JTynPi6XXfdNd8n/teoUaPc8OHDcytXrlzv/pSPUp4TS5cuzT+uWbNm+Xlw8cUX52699dZcjx498vHx48ev13NUblzxrqJ4IbKNN974y6sDS5cujVasWJG/QvDkk0+u9fj4LFOrVq3WuGrQpUuX6N57783/d9w/XgEwXkE2vpIQ3+IR/3vvvffyZ73i79pVtlBBfKYsvs0jPlO2LvEtIVdeeWX+at7RRx+dv1Xkpptuyu8j/l4flNuc+Lp47Lfccku0xx575M/6QrnNiU8//TT/M165+ZtWV79Y/RgohznxdTfeeGN033335T8vxceIeC6sXLky5TMBtXtOrL6tPN5ufMX+rLPOyu9zxowZUbt27b5cJ4c1Sbw3QJys7r777vkPIvEKr/FtFvELLl4F9puSShLtsssu+bp8sRdffDH/Qh8xYkR+O1//N3LkyPxj3nnnncx+lzgJj7+n8be//S2zfVD6SmFOPPLII/mDkkXVKNc5EX9vMPb555+v1fbZZ5+t8RgohznxdXHZpDiBOfXUU6P7778/mjx5cv6WXyinObH6GBBXvIi/S75anTp1oh/96EfR4sWLo1dffXWD91NqrGpeRfEbbbxsfnzm6Ve/+lV+gY34rNUFF1yQ//5bWvFZrlh8xih+Q0/SunXrKEvbbrtt/kwZlPOcmDJlSv7A0adPn4Jvm/JSW+dEvBhPfLX7zTffXKttdSyuZQzlMidC4vJi8fdr4+PGJZdcktl+KF21dU6sXrQtrtv9zXLE8e8Qe//996Pttttug/dVSiTeVRSv+BovTjZ16tSooqLiy/jqs0nfFN/a8U3//e9/ox122CH//+NtrT5zdNBBB0XVLT47Fp8ti2+vhXKdE/EVvjvvvDN/q5XEgnKdE/GJpw4dOkRz5sxZq2327Nn5ccSL6EC5zInKxLeaJ12ZhFI/Tnz729+OnnjiifzK7Ktvl4+98cYb+Z/xVXbW5FbzKlp9dufrpbjiDySPPfZY4uPvuuuuNb5TEa8aGD8+rou6+uxQ/GH/mmuuSbzKEK9wWKjl/5O2FdegjOM9evRYZ38otTmxWvwdqXjFWreZU+5zIr51MP5A9fXk+/nnn89/d7B3797r7A+lNieSbs+NL1jEKzsnVQCAUp8T8S3l8foG8a3yX/86UnwHSPw9bxcw1uaKdyVuuOGG/AIaSYuTHXbYYfmzU7169YoOPfTQaNGiRdH48ePzL7SkOnbxbR377rtv/jtB8VW1eEGz+HscZ5999pePGTduXP4x8ZWGk08+OX/WKi4PEE+++LsSzzzzTHCs8cTr1q1b/gzZuhZEiGv/xZMl3k98m8ijjz6aX0wqPnM1aNCg1M8T5aNU58Rq8cEivsU2XnQQynlOnHbaadGf/vSn/LjjWxbjqyeXXnpp1KJFi2jIkCGpnyfKR6nOiXj7cdmw+LNSfIt5fOXx+uuvj5YvXx5deOGFqZ8nykepzok4Z4gXVotLm8VX3ePbyidNmhS98sor0d133536eSoLxV5WvSYv/x/699prr+WX5R8zZkxu++23z22yySa5PfbYI3fPPffkfvKTn+Rj31z+P15mf+zYsbltt902//j99tsv98wzz6y175deeinXv3//3FZbbZWrV69erlWrVrnDDjssX/arUCUxBgwYkGvXrl2ucePG+X20bt069+tf/zr30UcfFeT5o/SU+pyIxaX06tevnzvqqKM2+Pmi9JXDnIh/h2OOOSa32Wab5cvFxPt44YUXNvi5ozSV+pyIH9O5c+dckyZN8iVYW7ZsmTvuuONyzz77bEGeP0pPqc+J2Ntvv50fa9OmTfPj6dKlS+6+++7b4OeuVFXE/1Ps5B8AAABKle94AwAAQIYk3gAAAJAhiTcAAABkSOINAAAAGZJ4AwAAQIYk3gAAAJAhiTcAAABkqO76PrCioiLLcUBRbEgZe3OCUmROQOHmhTlBKXKcgKrNC1e8AQAAIEMSbwAAAMiQxBsAAAAyJPEGAACADEm8AQAAIEMSbwAAAMiQxBsAAAAyJPEGAACADEm8AQAAIEMSbwAAAMiQxBsAAAAyJPEGAACADNXNcuMAAABUrzFjxgTb9tlnn2DbBRdckBifOXNmQcZVzlzxBgAAgAxJvAEAACBDEm8AAADIkMQbAAAAMiTxBgAAgAxV5HK53Ho9sKIiy3FAUaznyz+ROUEpMiegcPPCnKAUOU5Uv0aNGgXb7rjjjsT4/vvvH+yzySabBNu++OKLxPhOO+0U7PPmm29G5S63HvPCFW8AAADIkMQbAAAAMiTxBgAAgAxJvAEAACBDEm8AAADIkMQbAAAAMlQ3y40DAKVp4MCBwbZzzz03dQmbgw46KDE+d+7cKowO/l+nTp2CbZdeemlivG3btsE+nTt3Toy/+uqrVRgdrJ9u3bqlfu+sqnr16iXGzznnnGCfM844o6BjKFWueAMAAECGJN4AAACQIYk3AAAAZEjiDQAAABmSeAMAAECGrGoOAARdeeWVifFTTjkl2KdOnfTn9adPn54Y32mnnVJvi/LTq1evxPj48eODfZo1a5YYr6ioCPY5+eSTE+MjRoxY5xhhXQ4//PDE+E033VRtY1i4cGFifNy4cdU2hlLlijcAAABkSOINAAAAGZJ4AwAAQIYk3gAAAJAhiTcAAABkSOINAAAAGarI5XK59XpgJaUVKL6uXbsmxvfdd9/U23rttdeCbTfffHNUStbz5Z/InIii0aNHJ8aPO+64YJ/WrVsnxm+77bZgn7POOiv1a5WqMSdqv+985zvBtu222y4xfv755wf77LLLLgUrGVaZlStXJsYHDRoU7HPjjTdGNXlemBNVs8UWWyTGTz/99GCfYcOGpf4bhP6uVelz1113Bfv069cvMb5s2bKoNnKc2DCbb755sO3uu+9O9Tk/i7/DL3/5y8T4H/7wh4KOodSsz7xwxRsAAAAyJPEGAACADEm8AQAAIEMSbwAAAMiQxBsAAAAyZFXzjPXs2TMx3r59+2CfX/ziF6n3U79+/cR4w4YNU29r+fLlwbaPPvooMT5x4sRgnyFDhkQ1lZU5v9KgQYPE+HXXXRfs06tXr8T4xhtvHBXSkiVLEuOnnnpqsM+0adMKOoZyYU7UDpW9t995553Btu9///tRTbVixYrE+IknnhjsM2XKlKg6WNW8+lYuj917772pV+wv5Arlhe7Tv3//or5+C81xYsNWL7/66quDfY499tioOlxyySXBtuHDh6d6j+b/WdUcAAAAikziDQAAABmSeAMAAECGJN4AAACQIYk3AAAAZEjiDQAAABkq6XJinTt3DrYNHjw4Md6lS5eCjmHHHXdMjG+yySZRKfn000+DbT/84Q8T44888khUbEpifOWvf/1rYvzggw+OaqrK/n5z585NjI8ZMybY59FHH02Mv/HGG1G5MCdqR5m/sWPHBvsMHDgw9X7ef//91KWdKivZd+GFFybGW7duHezz+eefJ8b79OkT7POXv/wlqg7KiRXeqlWrClqya+rUqalKI8UWLFgQpfXEE0+k/sw5fvz41CUxazLHifXTvXv3xPj9999fLfufPHlysO2EE06oljGUk5xyYgAAAFBcEm8AAADIkMQbAAAAMiTxBgAAgAxJvAEAACBDdaNa4rjjjgu2hVaFbNeuXbBPs2bNolLy+uuvJ8YXL16celsLFy4Mtl199dWJ8RUrVgT7zJo1K/UYqH6dOnUq2Lbefffd1K/Vjh07FnR11A4dOiTGb7755tTjDq1IGxs5cmSlY4R1ady4cbDtkksuSYwPGDCgSvsKvVf36NEj2GfOnDmp93P22Wen7rN8+fLE+Msvv5x6W9TuFYBDbf379w/2Ca2yv2zZstRja9OmTeq2ylZpr6wCAKVrs802K2plodDxg+JxxRsAAAAyJPEGAACADEm8AQAAIEMSbwAAAMiQxBsAAAAyJPEGAACAcion1rt378T4NddcU6VSLNXhk08+Cba98847qbf34YcfJsbPO++8YJ/nn38+MT5v3rzU+6c8XXXVVanLAt17772J8ZNOOinY54svvkiM77jjjsE+Q4cOTYz37Nkz2Kdhw4ZRWs2bN0+Mn3LKKcE+yomxoSp7n27ZsmVB9xWas1UpGVZdJXGeeeaZah8L2evWrVvqOVFZqcpCOv/884NtDRo0SIzPnDkz2KeyNmq3evXqBduGDRtWsP1UVq7ulltuSYzPnTu3YPunMFzxBgAAgAxJvAEAACBDEm8AAADIkMQbAAAAMiTxBgAAgAxV5HK53Ho9sKKiYDs94ogjUq/MV79+/aiQpk6dmhifNWtW6m3Nnz8/2DZjxozU26P6rOfLP/M5UROE5ljHjh2DfWbPnh0VU/v27YNtF110UWK8R48eqfdT2Uq6LVq0iEqJObFh9t5772DbAw88UC3Ht1tvvTXY1q9fv8T4ypUrU++nV69ewbbbb7899WvkrrvuSowfffTRUW2dF+ZEYe2///7BtjZt2iTGlyxZEuzTrl27xPioUaNSvxZOO+20YJ9rr702KiWOExv2PlgVr7/+erBt++23L9h+yHZeuOINAAAAGZJ4AwAAQIYk3gAAAJAhiTcAAABkSOINAAAAGZJ4AwAAQKmVE6tslxtSoqBYnnvuudTlUWqCyy67LDG+dOnSqFwoiVG6OnfunBj/xz/+Eeyz8cYbJ8aVE1s/5TQnQmWNpk+fHuzTuHHjgu3/wQcfDLaNHDky2Pavf/2rYGMYPXp0sG3YsGGpt3fSSSclxidMmBAVm3JiNaNE05133pn6b1TZ36AqfUIlZHfbbbeoXDhOfGXevHnBtl133TWqqerUCV97XbVqVWL8qaeeCva54IILUs/ZUqOcGAAAABSZxBsAAAAyJPEGAACADEm8AQAAIEMSbwAAACi1Vc0rW1X4u9/9bsH2Q+WWL1+eGF+4cGGwz5gxYxLjkyZNimojK3OWruuuuy4x/tOf/jT1tp599tlg2x577BGVEnPiK9tuu22w7d///ndivFmzZgUdw8SJExPjp59+erDPxx9/XNAx9O7dO9XYKqsQUNnxpUuXLjW20oZVzQtviy22CLY9/vjjifHtttuuWlY1f++994J99txzz8T4q6++GpULx4n1q2xUk1c1r8q8qMyKFSsS45deemmwz1VXXZUYX7x4cVQbWdUcAAAAikziDQAAABmSeAMAAECGJN4AAACQIYk3AAAAZEjiDQAAABmqGxXBRRddFGzr06dPtYwhVA6idevWUbmoV69e6vIH119/feqSStdee21ifMGCBescI1RVIedyqIwepf1aGTJkSLBPIcuG9e3bN9h27733VkvJsMq0b98+VcmwyixatCjYVhPKhlF9KisNFmqrSimqqvSprNTZySefnBgfMWJE6v1AqahbNzmlPPvss4N9OnXqlBg/5JBDolLlijcAAABkSOINAAAAGZJ4AwAAQIYk3gAAAJAhiTcAAABkqCKXy+WyWhWyJtt2220T402bNo1KyT777BNsO+WUUxLju+++e0HH8OabbybGW7VqFRXber78y2JO1EYtW7YMts2ZMycx3qJFi2Cfp59+OjG+3377BfssW7YsKiWlOic23XTTYNuDDz6YGN9rr70KOoZbb701MT5gwIBqe32FqllMmDAh2OeYY45JtYptbPbs2am2FXvjjTeiUpsXNXlOFFuDBg1Sv37atWsX7DNv3rzE+IUXXph6bBMnTkz9Wgit/l+KVVxK9ThRFc8991ywrbIqQcU2f/78YFvbtm2rZQyPP/54Yrxr165Rqc4LV7wBAAAgQxJvAAAAyJDEGwAAADIk8QYAAIAMSbwBAAAgQxJvAAAAyFC4FkiJe+2111LFa6tnnnkm2DZt2rTEeN++fYN9xo4dm3oMW2+9deo+sD4uvvjiYFtlZcNCZs6cWRYlw0rZRhttlBi/4oorgn0KWTZs+vTpwbZ+/folxleuXBlVR8mwysp5HXfccQU9vhx11FGJ8bfeeiv1fihNlb2vdujQISqmSZMmpe6z//77l005MWqW0Of5iy66KNjn9ddfD7aFyv3ec889wT7NmzeP0tptt90S4wcccECwzyOPPBLVZq54AwAAQIYk3gAAAJAhiTcAAABkSOINAAAAGZJ4AwAAQIbKdlVzoujtt99OjD/55JPVPhaoTJMmTRLjvXv3Tr2tl156Kdg2fvz41NujZunfv39i/MQTT6yWVWRDq4ZXpwkTJgTbqrJ6eci1114bbLN6ObVZLperUhsUw+GHH54Y33zzzYN9hg8fHmwbMWJEYrxx48ZRITVs2DAxfuyxxwb7WNUcAAAACJJ4AwAAQIYk3gAAAJAhiTcAAABkSOINAAAAGZJ4AwAAQIZKupxY8+bNg2277bZbYvyf//xnsM+KFSsKMi5gbc2aNQu23XXXXYnxjTbaKPV+rr/++mDbK6+8knp71CyjRo0q2LYWLVoUbOvTp09UHbp165ZqTsTq169f0DEMGjQoMX7DDTcUdD/UDG3atAm2LViwIColAwcOTIxXVFSk3tbf//73AoyI2mby5MnBttGjR1fLGOrWTU7nDjzwwGCfxx57rOgl8z799NPE+NSpU6NS5Yo3AAAAZEjiDQAAABmSeAMAAECGJN4AAACQIYk3AAAAZKikVzXffPPNg20PPfRQYvyBBx4I9unRo0difOXKlVEp6d69e7GHQBnafffdg21du3Yt2H6sxFz7XXHFFcG2rbbaqmD7ufDCC4Nty5cvT729Xr16JcZHjBgR7NOiRYvEeKNGjaKqWLVqVWL85z//eeo5E9oWtcO5556bGB8wYECwz5577pkYf/fdd6PauEr70KFDU6/qPG/evLJY8Z31M2XKlGDbMccckxjv2LFjhiOqPf4ZqCRVWS5W27niDQAAABmSeAMAAECGJN4AAACQIYk3AAAAZEjiDQAAABmSeAMAAECGKnKV1Uz4+gMrKqLaZosttgi2Pfzww4nxtm3bBvuEyq1MmDAh2Od///tfVFM9++yzqZ+DjTbaKPV+Zs2alXmJqKpaz5d/ycyJmqBOneTzfffdd19BS9zddNNNifFBgwYVtERUqakNc6KyMRayxNWLL74YbFu6dGnq7YVKMRX6eZs9e3aw7fTTT0+Mz5kzp6BjKDVVnRfFPk6EyqDGZsyYkaoEUmzatGlRMTVs2DB1ub6JEyem/rsuW7Ys9Twup3JiteE4URNss802ifG777472KdDhw5Rdajs77Ahf9/1zTViRxxxRJRk8eLFUW20Ps+bK94AAACQIYk3AAAAZEjiDQAAABmSeAMAAECGJN4AAACQoZJe1bwygwcPToxfeeWVqbc1ffr0YNukSZMS448++miwz9tvv516RfEjjzwyMT569Ohgn9atWyfG69atG6X1yCOPpF5p9IMPPoiKzcqc1W/AgAGJ8WuuuSb1tt59991gW5cuXRLjL7/8cur9lJPaMCeqa1XzmmDhwoWJ8XHjxgX73HbbbcG2N954oyDjKje1dVXz8ePHp34vbt++fbBPda3cHfrccPzxxwf79OzZs2CrN/fu3bvGruxeE9SG40RN1rJly2Bbv379EuONGjUK9hk6dGhB/w6/+93vEuMHHHBAsM/MmTNTVxV48803o1JiVXMAAAAoMok3AAAAZEjiDQAAABmSeAMAAECGJN4AAACQIYk3AAAAZKhsy4mFSnN16tQp2CdUAqwq5beef/75YNuHH36YGK9TJ3yepHPnzlF1ePDBBxPjffr0CfZZsmRJVFMpiZGNJk2aBNvuvPPO1GUqQs4+++xg29ixY1Nvj9oxJ955551gW9OmTWvsfA2Vvzv//PODfSZPnpwYX7p0acHGRemWE6uspOiwYcNSf9YIvX+/9957qZ+7gQMHpu5Tlf2MGDEi2GfevHmpS75SO44TUN2UEwMAAIAik3gDAABAhiTeAAAAkCGJNwAAAGRI4g0AAAAZKttVzasitPJs//79g31atWoV1Tahlctjffv2Tb3KcE1mZc5sjB8/Pth28sknp97eihUrEuMNGzZM3YfSnhODBw9OjA8fPjzYZ8stt0y9gvLvf//71GO77LLLEuMrV65MvS2qV21d1bxBgwbBtqFDhybGjzrqqGCfXXfdNfXvGXruqtLn0EMPDfaZP39+YvzVV18N9qE8jxOQBauaAwAAQJFJvAEAACBDEm8AAADIkMQbAAAAMiTxBgAAgAxJvAEAACBDyokVwE477RRs23///VNvL1Syq2PHjqm3NWvWrGDbtGnTEuMzZswI9lmyZElUSpTEyKZUzdy5c4N9tt9++9T7GTNmTGJ8xIgRqbdF5cwJKJ1yYpAFxwlYm3JiAAAAUGQSbwAAAMiQxBsAAAAyJPEGAACADEm8AQAAIENWNaesWZlz3bbbbrtgW2hV8RNPPDH1fm6//fZgW2h7y5YtS70fKmdOwNqsag5fcZyAtVnVHAAAAIpM4g0AAAAZkngDAABAhiTeAAAAkCGJNwAAAGRI4g0AAAAZUk6MsqYkBqzJnIC1KScGX3GcgLUpJwYAAABFJvEGAACADEm8AQAAIEMSbwAAAMiQxBsAAAAyJPEGAACADEm8AQAAIEMSbwAAAMiQxBsAAAAyJPEGAACADEm8AQAAIEMSbwAAAMhQRS6Xy2W5AwAAAChnrngDAABAhiTeAAAAkCGJNwAAAGRI4g0AAAAZkngDAABAhiTeAAAAkCGJNwAAAGRI4g0AAAAZkngDAABAlJ3/A+bwVIOcSEeEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# display 5 samples images\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in tqdm(range(5)):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(images[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title(f'Label: {labels[i].item()}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].numpy().squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DigitRecognizer(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DigitRecognizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitRecognizer, self).__init__()\n",
    "\n",
    "        # Input image asre 28x28 pixels = 784 input features\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(28*28, 128) # 784 -> 128\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)   # 128 -> 64\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 10)    # 64 -> 10 (one for each digit 0-9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 1, 28, 28]\n",
    "        x = self.flatten(x)             # Flatten to [batch_size, 784]\n",
    "        x = self.fc1(x)                # First fully connected layer\n",
    "        x = self.relu1(x)             # Apply ReLU activation\n",
    "        x = self.fc2(x)                # Second fully connected layer\n",
    "        x = self.relu2(x)             # Apply ReLU activation\n",
    "        x = self.fc3(x)                # Third fully connected layer\n",
    "        return x\n",
    "    \n",
    "# Create the model\n",
    "model = DigitRecognizer()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy data shape:  torch.Size([1, 1, 28, 28])\n",
      "Dummy output shape:  torch.Size([1, 10])\n",
      "Dummy output:  tensor([[ 0.0291,  0.1006, -0.0768,  0.1400, -0.0904,  0.0068, -0.1537,  0.2031,\n",
      "         -0.0491, -0.0850]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# create dummpy data to test the model\n",
    "dummy_data = torch.rand((1, 1, 28, 28))  # Batch size of 1, 1 channel, 28x28 image\n",
    "dummy_output = model(dummy_data)\n",
    "print(\"Dummy data shape: \", dummy_data.shape)  # Should be [1, 1, 28, 28]\n",
    "# print(\"Dummy data: \", dummy_data)\n",
    "print(\"Dummy output shape: \", dummy_output.shape)  # Should be [1, 10]\n",
    "print(\"Dummy output: \", dummy_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function and Optimizer\n",
    "\n",
    "Before training, we need to define:\n",
    "\n",
    "- A loss function to measure prediction errors\n",
    "- An optimizer to update model weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entropy loss function is standard for classification tasks\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create optimizer : SGD optimizer with learning rate of 0.01 and momentum of 0.9\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 1.078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 200, Loss: 0.361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 300, Loss: 0.303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 400, Loss: 0.255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 500, Loss: 0.224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 600, Loss: 0.196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 700, Loss: 0.186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 800, Loss: 0.170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:03, 250.80it/s]\n",
      " 20%|██        | 1/5 [00:03<00:15,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 900, Loss: 0.160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 100, Loss: 0.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 200, Loss: 0.120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 300, Loss: 0.116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 400, Loss: 0.121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 500, Loss: 0.113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 600, Loss: 0.116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 700, Loss: 0.120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 800, Loss: 0.116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:03, 264.79it/s]\n",
      " 40%|████      | 2/5 [00:07<00:10,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 900, Loss: 0.096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 100, Loss: 0.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 200, Loss: 0.088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 300, Loss: 0.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 400, Loss: 0.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 500, Loss: 0.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 600, Loss: 0.079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 700, Loss: 0.074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 800, Loss: 0.073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:03, 266.34it/s]\n",
      " 60%|██████    | 3/5 [00:10<00:07,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 900, Loss: 0.078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 100, Loss: 0.055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 200, Loss: 0.059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 300, Loss: 0.061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 400, Loss: 0.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 500, Loss: 0.065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 600, Loss: 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 700, Loss: 0.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 800, Loss: 0.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:03, 269.07it/s]\n",
      " 80%|████████  | 4/5 [00:14<00:03,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 900, Loss: 0.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 100, Loss: 0.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 200, Loss: 0.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 300, Loss: 0.048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 400, Loss: 0.048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 500, Loss: 0.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 600, Loss: 0.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 700, Loss: 0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 800, Loss: 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:03, 271.82it/s]\n",
      "100%|██████████| 5/5 [00:17<00:00,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 900, Loss: 0.051\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in tqdm(enumerate(train_loader, 0)):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99: # Print every 100 mini-batches\n",
    "            print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:00<00:00, 217.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 97.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set model to evalutation mode\n",
    "model.eval()\n",
    "\n",
    "# Variables to track accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# No gradient compuatation needed for evaluation\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        images, labels = data\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update statistics\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels ).sum().item()\n",
    "\n",
    "print(f'Accuracy on the test set: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing torch.max\n",
    "ls = [1, 2, 3]\n",
    "ls_tensor = torch.tensor(ls)\n",
    "torch.max(ls_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Prediction on New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 1, 28, 28])\n",
      "Predicted digit: 7\n"
     ]
    }
   ],
   "source": [
    "def preprocess_image(image_path):\n",
    "    \n",
    "    # Open and convert to grayscale\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    \n",
    "    # Resize to 28x28 pixels\n",
    "    image = image.resize((28, 28))\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    image_tensor = transforms.ToTensor()(image)\n",
    "    image_tensor = transforms.Normalize((0.1307,), (0.3081,))(image_tensor)\n",
    "    \n",
    "    print(image_tensor.shape)  # Now prints: torch.Size([1, 1, 28, 28])\n",
    "    # Add batch dimension\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    print(image_tensor.shape)  # Might print: torch.Size([1, 28, 28])\n",
    "    \n",
    "    return image_tensor\n",
    "\n",
    "\n",
    "# Func to predict degit\n",
    "def predict_digit(model, image_tensor):\n",
    "    # Set model to evalation model\n",
    "    model.eval()\n",
    "\n",
    "    # Get the prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "# Example usage\n",
    "image_tensor = preprocess_image('./digit-one.png')\n",
    "result = predict_digit(model, image_tensor=image_tensor)\n",
    "print(f'Predicted digit: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DigitRecognizer(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pth_path = 'digit_recognizer_v1.pth'\n",
    "\n",
    "torch.save(model.state_dict(), model_pth_path)\n",
    "\n",
    "loaded_model = DigitRecognizer()\n",
    "loaded_model.load_state_dict(torch.load(model_pth_path))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 109,386\n",
      "fc1.weight: torch.Size([128, 784]), Parameters: 100,352\n",
      "fc1.bias: torch.Size([128]), Parameters: 128\n",
      "fc2.weight: torch.Size([64, 128]), Parameters: 8,192\n",
      "fc2.bias: torch.Size([64]), Parameters: 64\n",
      "fc3.weight: torch.Size([10, 64]), Parameters: 640\n",
      "fc3.bias: torch.Size([10]), Parameters: 10\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage\n",
    "total_params = count_parameters(model)\n",
    "print(f'Total trainable parameters: {total_params:,}')\n",
    "\n",
    "\n",
    "# To see parameter sizes in each layer\n",
    "for name, parameter in model.named_parameters():\n",
    "    if parameter.requires_grad:\n",
    "        print(f\"{name}: {parameter.shape}, Parameters: {parameter.numel():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Model Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight: torch.float32\n",
      "fc1.bias: torch.float32\n",
      "fc2.weight: torch.float32\n",
      "fc2.bias: torch.float32\n",
      "fc3.weight: torch.float32\n",
      "fc3.bias: torch.float32\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {parameter.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covert model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to ONNX format!\n"
     ]
    }
   ],
   "source": [
    "model_onnx_path = model_pth_path.replace(\".pth\", \".onnx\")\n",
    "\n",
    "# First, create a dummy input tensor matching your expected input shape\n",
    "# For MNIST, this would be a batch of images with shape [batch_size, channels, height, width]\n",
    "dummy_input = torch.randn(1, 1, 28, 28)  # Batch size 1, 1 channel, 28x28 pixels\n",
    "\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    model_onnx_path,\n",
    "    opset_version=13,      # ONNX version\n",
    "    do_constant_folding=True,  # Optimize constant folding\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},\n",
    "        'ouput': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print('Model exported to ONNX format!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 2\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Create an ONNX Runtime session\n",
    "session = ort.InferenceSession(model_onnx_path)\n",
    "\n",
    "# Prepare input\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_data = dummy_input.numpy()  # Convert tensor to numpy\n",
    "\n",
    "# Run inference\n",
    "outputs = session.run(None, {input_name: input_data})\n",
    "predicted_class = outputs[0].argmax(axis=1)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 1, 28, 28])\n",
      "Predicted class: 7\n"
     ]
    }
   ],
   "source": [
    "image_tensor = preprocess_image('./digit-8.png')\n",
    "input_data = image_tensor.numpy()\n",
    "\n",
    "outputs = session.run(None, {input_name: input_data})\n",
    "predicted_class = outputs[0].argmax(axis=1)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize model to half precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half precision model saved to: digit_recognizer_v1_fp16.onnx\n",
      "ONNX FP16 model is valid!\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxmltools.utils.float16_converter import convert_float_to_float16\n",
    "\n",
    "onnx_model = onnx.load(model_onnx_path)\n",
    "\n",
    "# Convert the ONNX model to FP16\n",
    "onnx_fp16_path = model_onnx_path.replace(\".onnx\", \"_fp16.onnx\")\n",
    "onnx_model_fp16 = convert_float_to_float16(onnx_model)\n",
    "\n",
    "# Save the FP16 model\n",
    "onnx.save(onnx_model_fp16, onnx_fp16_path)\n",
    "print(f\"Half precision model saved to: {onnx_fp16_path}\")\n",
    "\n",
    "# Verity the model\n",
    "onnx.checker.check_model(onnx_model_fp16)\n",
    "print(\"ONNX FP16 model is valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 1, 28, 28])\n",
      "Predicted class: 7\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Create an ONNX Runtime session\n",
    "session = ort.InferenceSession(onnx_fp16_path)\n",
    "\n",
    "# Prepare input\n",
    "input_name = session.get_inputs()[0].name\n",
    "image_tensor = preprocess_image('./digit-one.png')\n",
    "image_tensor = image_tensor.half() # convert to half precision\n",
    "image_array = image_tensor.numpy()\n",
    "\n",
    "# Run inference\n",
    "outputs = session.run(None, {input_name: image_array})\n",
    "predicted_class = outputs[0].argmax(axis=1)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
